{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> # Paolo Moriello, Giuseppe Coccia\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288857968\n",
      "Epoch:  02   =====> Loss= 0.732793764\n",
      "Epoch:  03   =====> Loss= 0.600320643\n",
      "Epoch:  04   =====> Loss= 0.536872040\n",
      "Epoch:  05   =====> Loss= 0.497822908\n",
      "Epoch:  06   =====> Loss= 0.471331935\n",
      "Epoch:  07   =====> Loss= 0.451140417\n",
      "Epoch:  08   =====> Loss= 0.436008948\n",
      "Epoch:  09   =====> Loss= 0.423498562\n",
      "Epoch:  10   =====> Loss= 0.412933110\n",
      "Epoch:  11   =====> Loss= 0.404503482\n",
      "Epoch:  12   =====> Loss= 0.396694267\n",
      "Epoch:  13   =====> Loss= 0.390278287\n",
      "Epoch:  14   =====> Loss= 0.384612805\n",
      "Epoch:  15   =====> Loss= 0.379301537\n",
      "Epoch:  16   =====> Loss= 0.374702650\n",
      "Epoch:  17   =====> Loss= 0.370602037\n",
      "Epoch:  18   =====> Loss= 0.366252979\n",
      "Epoch:  19   =====> Loss= 0.362944725\n",
      "Epoch:  20   =====> Loss= 0.359408229\n",
      "Epoch:  21   =====> Loss= 0.356740769\n",
      "Epoch:  22   =====> Loss= 0.353887012\n",
      "Epoch:  23   =====> Loss= 0.351285579\n",
      "Epoch:  24   =====> Loss= 0.348614941\n",
      "Epoch:  25   =====> Loss= 0.346529221\n",
      "Epoch:  26   =====> Loss= 0.344402499\n",
      "Epoch:  27   =====> Loss= 0.342044957\n",
      "Epoch:  28   =====> Loss= 0.340140978\n",
      "Epoch:  29   =====> Loss= 0.338248981\n",
      "Epoch:  30   =====> Loss= 0.336668299\n",
      "Epoch:  31   =====> Loss= 0.335115347\n",
      "Epoch:  32   =====> Loss= 0.333498004\n",
      "Epoch:  33   =====> Loss= 0.332095947\n",
      "Epoch:  34   =====> Loss= 0.330456927\n",
      "Epoch:  35   =====> Loss= 0.329246766\n",
      "Epoch:  36   =====> Loss= 0.327942810\n",
      "Epoch:  37   =====> Loss= 0.326455839\n",
      "Epoch:  38   =====> Loss= 0.325407386\n",
      "Epoch:  39   =====> Loss= 0.324123150\n",
      "Epoch:  40   =====> Loss= 0.323043245\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9157\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet5_Model(image):    \n",
    "    # your inmplementation goes here\n",
    "\n",
    "    # Layer 1: Convolutional. Input = 28x28x1. Output = 28x28x6.\n",
    "    conv1_W = weight_variable(shape=(5, 5, 1, 6))\n",
    "    conv1_b = bias_variable(shape = [6])\n",
    "    conv1_output = tf.nn.conv2d(image, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    # Activation.\n",
    "    conv1_output = tf.nn.relu(conv1_output)\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1_output = tf.nn.max_pool(conv1_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Input = 14x14x6. Output = 10x10x16.\n",
    "    conv2_W = weight_variable(shape=(5, 5, 6, 16)) \n",
    "    conv2_b = bias_variable(shape = [16])\n",
    "    conv2_output   = tf.nn.conv2d(conv1_output, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    # Activation.\n",
    "    conv2_output = tf.nn.relu(conv2_output)\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2_output = tf.nn.max_pool(conv2_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0_output = flatten(conv2_output)  #tf.reshape(conv2_output, [-1])   \n",
    "\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = weight_variable(shape=(400, 120)) \n",
    "    fc1_b = bias_variable(shape = [120])\n",
    "    fc1_output = tf.matmul(fc0_output, fc1_W) + fc1_b\n",
    "    # Activation.\n",
    "    fc1_output = tf.nn.relu(fc1_output)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W = weight_variable(shape=(120, 84)) \n",
    "    fc2_b = bias_variable(shape = [84])\n",
    "    fc2_output = tf.matmul(fc1_output, fc2_W) + fc2_b\n",
    "    # Activation.\n",
    "    fc2_output = tf.nn.relu(fc2_output)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W = weight_variable(shape=(84, 10))\n",
    "    fc3_b = bias_variable(shape = [10])\n",
    "    fc3_output = tf.matmul(fc2_output, fc3_W) + fc3_b\n",
    "\n",
    "    return fc3_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59706\n"
     ]
    }
   ],
   "source": [
    "# conv l1\n",
    "conv1 = 5*5*1*6 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# conv l2\n",
    "conv2 = 5*5*1*16 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# fcl1\n",
    "fcl1 = 5*5*16*120 # fcl_input_size * fcl_output_size\n",
    "# fcl2\n",
    "fcl2 = 84*120 # fcl_input_size * fcl_output_size\n",
    "# fcl3\n",
    "fcl3 = 84*10 # fcl_input_size * fcl_output_size\n",
    "# biases\n",
    "bias = 6+16+120+84+10\n",
    "\n",
    "total = bias + fcl1 + fcl2 + fcl3 + conv2 + conv1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files/'\n",
    "display_step = 10\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, (None, 28, 28, 1), name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.int32, [None, 10], name='LabelData')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = LeNet5_Model(x)\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 128\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(logits, labels):\n",
    "    # logits will be the outputs of your model, labels will be one-hot vectors corresponding to the actual labels\n",
    "    # logits and labels are numpy arrays\n",
    "    # this function should return the accuracy of your model\n",
    "    num_examples = len(logits)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    for offset in range(0, num_examples, VALIDATION_SIZE):\n",
    "        batch_x, batch_y = logits[offset:offset+VALIDATION_SIZE], labels[offset:offset+VALIDATION_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        \n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (55000, 28, 28, 1))\n",
    "X_validation = np.reshape(X_validation, (5000, 28, 28, 1))\n",
    "X_test = np.reshape(X_test, (10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(init, sess, logs_path, n_epochs, batch_size, optimizer, cost, merged_summary_op):\n",
    "    # optimizer and cost are the same kinds of objects as in Section 1\n",
    "    # Train your model\n",
    "    global X_train, y_train, X_validation, y_validation\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) if mnist.train.num_examples%batch_size == 0 else int(mnist.train.num_examples/batch_size)+1\n",
    "        # Loop over all batches\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, len(X_train), batch_size):\n",
    "            batch_xs, batch_ys = X_train[offset:offset+batch_size], y_train[offset:offset+batch_size]\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  ==> Loss:\", \"{:.9f}\".format(avg_cost),\n",
    "                  \" ==> Training Accuracy:\", evaluate(X_train, y_train),\n",
    "                 \" ==> Validation Accuracy:\", evaluate(X_validation, y_validation))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10   ==> Loss: 0.389845026  ==> Training Accuracy: 0.8919636363636364  ==> Validation Accuracy: 0.8954\n",
      "Epoch:  20   ==> Loss: 0.244048110  ==> Training Accuracy: 0.9277818181731484  ==> Validation Accuracy: 0.9328\n",
      "Epoch:  30   ==> Loss: 0.189173302  ==> Training Accuracy: 0.9438909090475602  ==> Validation Accuracy: 0.9492\n",
      "Epoch:  40   ==> Loss: 0.155106402  ==> Training Accuracy: 0.953818181774833  ==> Validation Accuracy: 0.9578\n",
      "Optimization Finished!\n",
      "Model saved\n",
      "Training time: 615.5931944847107\n",
      "Test Accuracy: 0.9546\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_LeNet-5_SGD\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_LeNet-5_SGD\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    t0 = time.time()\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # saving model\n",
    "    saver.save(sess, './LeNet_SGD')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    print(\"Training time:\", t1-t0)\n",
    "    \n",
    "    # Test model\n",
    "    # Print the accuracy on testing data\n",
    "    print(\"Test Accuracy:\", acc.eval({x: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have printed out the stats every 10 epochs, instead of every 100, because we have a total of 40 epochs, and it would not have been shown anything otherwise. As we can see, there is an improvement from epoch to epoch: the loss always decreases, and the training and validation accuracies always increase.\n",
    "\n",
    "The final obtained accuracy on the test set with this first Lenet implementation is 95.46%, which is already an improvement with respect to the result above. This confirms the fact that the LeNet architecture perfectly fits the given case of study.\n",
    "\n",
    "Now let's see what happens with a more optimized implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MNIST_figures/acc.png\",width=\"800\" height=\"600\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MNIST_figures/loss.png\",width=\"800\" height=\"600\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |       95,46%       |        99,29%       |       \n",
    "| Training Time        |       615,6s       |        622,6s       |       \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "**Your answer:** ADAM. The Adam Optimizer is able to reach the increadible 99.3% of accuracy. The training time is more or less the same in both the cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10   ==> Loss: 0.018782492  ==> Training Accuracy: 0.9947272727446123  ==> Validation Accuracy: 0.9884\n",
      "Epoch:  20   ==> Loss: 0.007352517  ==> Training Accuracy: 0.9988727272727272  ==> Validation Accuracy: 0.9918\n",
      "Epoch:  30   ==> Loss: 0.005515630  ==> Training Accuracy: 0.9987636363636364  ==> Validation Accuracy: 0.9924\n",
      "Epoch:  40   ==> Loss: 0.003077110  ==> Training Accuracy: 0.9996727272727273  ==> Validation Accuracy: 0.9906\n",
      "Optimization Finished!\n",
      "Model saved\n",
      "Training time: 622.5887453556061\n",
      "Test Accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, (None, 28, 28, 1), name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.int32, [None, 10], name='LabelData')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = LeNet5_Model(x)\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "with tf.name_scope('Adam'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    \n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_LeNet-5_Adam\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_LeNet-5_Adam\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    t0 = time.time()\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # saving model\n",
    "    saver.save(sess, './LeNet_Adam')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    print(\"Training time:\", t1-t0)\n",
    "    \n",
    "    # Test model\n",
    "    # Print the accuracy on testing data\n",
    "    print(\"Test Accuracy:\", acc.eval({x: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have reached our goal: ~99,3% accuracy!\n",
    "\n",
    "The Adam is different to classical stochastic gradient descent. As we know, Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.\n",
    "\n",
    "In this case, instead, a learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds: \"the method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients\".\n",
    "\n",
    "So, thanks to the AdamOptimizer, we are able to reach the impressive 99% accuracy on the test set!\n",
    "\n",
    "We can see the effectiveness of this application by looking also at the results obtained during the training. At the first print (epoch 10) we have already an accuracy and a loss which are way better than the ones obtained during the training using the SGD. The training accuracy is always over the 99% (at the end after 40 iterations it is 99.97% !!!), and the validation accuracy comes at that point right after the first 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** 98,86%\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. This reliant on context for a neuron during training is referred to complex co-adaptations.\n",
    "\n",
    "If neurons are randomly dropped out of the network during training, other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\n",
    "\n",
    "The effect is that **the network becomes less sensitive to the specific weights of neurons**. This in turn results in a network that is capable of **better generalization** and is **less likely to overfit** the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout(image):    \n",
    "    # Layer 1: Convolutional. Input = 28x28x1. Output = 28x28x6.\n",
    "    conv1_W = weight_variable(shape=(5, 5, 1, 6))\n",
    "    conv1_b = bias_variable(shape = [6])\n",
    "    conv1_output = tf.nn.conv2d(image, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    # Activation.\n",
    "    conv1_output = tf.nn.relu(conv1_output)\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1_output = tf.nn.max_pool(conv1_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Input = 14x14x6. Output = 10x10x16.\n",
    "    conv2_W = weight_variable(shape=(5, 5, 6, 16)) \n",
    "    conv2_b = bias_variable(shape = [16])\n",
    "    conv2_output   = tf.nn.conv2d(conv1_output, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    # Activation.\n",
    "    conv2_output = tf.nn.relu(conv2_output)\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2_output = tf.nn.max_pool(conv2_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0_output = flatten(conv2_output)  #tf.reshape(conv2_output, [-1])\n",
    "    \n",
    "    # adding dropout\n",
    "    fc0_output = tf.nn.dropout(fc0_output, keep_prob=0.75)\n",
    "\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = weight_variable(shape=(400, 120)) \n",
    "    fc1_b = bias_variable(shape = [120])\n",
    "    fc1_output = tf.matmul(fc0_output, fc1_W) + fc1_b\n",
    "    # Activation.\n",
    "    fc1_output = tf.nn.relu(fc1_output)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W = weight_variable(shape=(120, 84)) \n",
    "    fc2_b = bias_variable(shape = [84])\n",
    "    fc2_output = tf.matmul(fc1_output, fc2_W) + fc2_b\n",
    "    # Activation.\n",
    "    fc2_output = tf.nn.relu(fc2_output)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W = weight_variable(shape=(84, 10))\n",
    "    fc3_b = bias_variable(shape = [10])\n",
    "    fc3_output = tf.matmul(fc2_output, fc3_W) + fc3_b\n",
    "\n",
    "    return fc3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10   ==> Loss: 0.035443074  ==> Training Accuracy: 0.9878727272900668  ==> Validation Accuracy: 0.9826\n",
      "Epoch:  20   ==> Loss: 0.019718802  ==> Training Accuracy: 0.9935636363636363  ==> Validation Accuracy: 0.9872\n",
      "Epoch:  30   ==> Loss: 0.014319925  ==> Training Accuracy: 0.9962545454545455  ==> Validation Accuracy: 0.9888\n",
      "Epoch:  40   ==> Loss: 0.011600548  ==> Training Accuracy: 0.9965272727619517  ==> Validation Accuracy: 0.9892\n",
      "Optimization Finished!\n",
      "Model saved\n",
      "Training time: 625.9656348228455\n",
      "Test Accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, (None, 28, 28, 1), name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.int32, [None, 10], name='LabelData')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = LeNet5_Model_Dropout(x)\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "with tf.name_scope('Adam'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    \n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_LeNet-5_Adam_Dropout\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_LeNet-5_Adam_Dropout\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    t0 = time.time()\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # saving model\n",
    "    saver.save(sess, './LeNet_Adam_Dropout')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    print(\"Training time:\", t1-t0)\n",
    "    \n",
    "    # Test model\n",
    "    # Print the accuracy on testing data\n",
    "    print(\"Test Accuracy:\", acc.eval({x: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case we have a very good result, with a final test accuracy of 0.988. Altought it is less than before, it is still very very good, an close to 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
